{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install torch opencv-python numpy moviepy together openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "from moviepy.editor import VideoFileClip\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng API LLama 3.2 để trích xuất đặc trưng từ video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from together import Together\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import whisper\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "import numpy as np\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Initialize Together client\n",
    "client = Together(api_key=\"\")\n",
    "\n",
    "def convert_image_to_base64(image):\n",
    "    \"\"\"\n",
    "    Convert PIL Image to base64 string.\n",
    "    \"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='JPEG', quality=85)  # Save with high efficiency\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "def call_together_ai_api(image, query):\n",
    "    \"\"\"\n",
    "    Sends an image to TogetherAI API for caption generation.\n",
    "    \"\"\"\n",
    "    base64_image = convert_image_to_base64(image)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating caption: {str(e)}\"\n",
    "\n",
    "# Function to interpret acoustic features into natural language description\n",
    "def interpret_acoustic_features(y, sr):\n",
    "    \"\"\"\n",
    "    Phân tích và diễn giải các đặc trưng âm thanh thành mô tả ngôn ngữ tự nhiên.\n",
    "    \n",
    "    Args:\n",
    "        y (np.ndarray): Tín hiệu âm thanh\n",
    "        sr (int): Tần số lấy mẫu\n",
    "        \n",
    "    Returns:\n",
    "        str: Mô tả bằng ngôn ngữ tự nhiên về đặc điểm âm thanh\n",
    "    \"\"\"\n",
    "    if len(y) == 0 or len(y) < sr/10:\n",
    "        return \"đoạn âm thanh quá ngắn để phân tích\"\n",
    "        \n",
    "    try:\n",
    "        description = []\n",
    "        \n",
    "        # Chuẩn hóa âm thanh để đảm bảo tính nhất quán\n",
    "        y = librosa.util.normalize(y)\n",
    "        \n",
    "        # Phân tích MFCC với window và hop length phù hợp\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, \n",
    "                                  n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "\n",
    "        # Phân tích tông giọng với ngưỡng được tinh chỉnh\n",
    "        if mfcc_mean[0] > 50:\n",
    "            description.append(\"tông giọng cao\")\n",
    "        elif mfcc_mean[0] < -50:\n",
    "            description.append(\"tông giọng trầm\")\n",
    "        else:\n",
    "            description.append(\"tông giọng trung bình\")\n",
    "\n",
    "        # Phân tích ZCR với frame size phù hợp\n",
    "        if len(y) >= n_fft:\n",
    "            zcr = librosa.feature.zero_crossing_rate(y=y, \n",
    "                                                   frame_length=n_fft, \n",
    "                                                   hop_length=hop_length)\n",
    "            zcr_mean = np.mean(zcr)\n",
    "            if zcr_mean > 0.1:\n",
    "                description.append(\"nói nhanh và mạnh\")\n",
    "            else:\n",
    "                description.append(\"nói chậm rãi và êm dịu\")\n",
    "\n",
    "        # Phân tích năng lượng RMS với frame size nhất quán\n",
    "        rms = librosa.feature.rms(y=y, frame_length=n_fft, hop_length=hop_length)\n",
    "        rms_mean = np.mean(rms)\n",
    "        if rms_mean > 0.1:  # Điều chỉnh ngưỡng sau khi chuẩn hóa\n",
    "            description.append(\"nói lớn và mạnh mẽ\")\n",
    "        else:\n",
    "            description.append(\"nói nhẹ nhàng\")\n",
    "\n",
    "        # Phân tích spectral centroid\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=sr, \n",
    "                                                   n_fft=n_fft, \n",
    "                                                   hop_length=hop_length)\n",
    "        centroid_mean = np.mean(centroid)\n",
    "        # Điều chỉnh ngưỡng dựa trên tần số lấy mẫu\n",
    "        centroid_threshold = sr / 6\n",
    "        if centroid_mean > centroid_threshold:\n",
    "            description.append(\"giọng sáng và rõ nét\")\n",
    "        else:\n",
    "            description.append(\"giọng trầm và ấm áp\")\n",
    "\n",
    "        # Phân tích spectral bandwidth với parameters nhất quán\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr,\n",
    "                                                     n_fft=n_fft,\n",
    "                                                     hop_length=hop_length)\n",
    "        bandwidth_mean = np.mean(bandwidth)\n",
    "        # Điều chỉnh ngưỡng dựa trên tần số lấy mẫu\n",
    "        bandwidth_threshold = sr / 8\n",
    "        if bandwidth_mean > bandwidth_threshold:\n",
    "            description.append(\"giọng sôi nổi và nhiều âm sắc\")\n",
    "        else:\n",
    "            description.append(\"giọng đều đặn và ít biến động\")\n",
    "            \n",
    "        # Thêm phân tích tempo\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        if tempo > 120:\n",
    "            description.append(\"nhịp độ nói nhanh\")\n",
    "        elif tempo < 80:\n",
    "            description.append(\"nhịp độ nói chậm\")\n",
    "        else:\n",
    "            description.append(\"nhịp độ nói vừa phải\")\n",
    "\n",
    "        return \", \".join(description)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"lỗi khi phân tích: {str(e)}\"\n",
    "\n",
    "def extract_frames(video_path, num_frames=10):\n",
    "    \"\"\"\n",
    "    Extract evenly spaced frames from a video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Could not open video file.\")\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if num_frames > total_frames:\n",
    "        num_frames = total_frames\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def frames_to_pil(frames):\n",
    "    \"\"\"\n",
    "    Convert OpenCV frames to PIL Image objects.\n",
    "    \"\"\"\n",
    "    return [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in frames]\n",
    "\n",
    "def extract_audio(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extract audio from a video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        video.audio.write_audiofile(audio_path, fps=16000, nbytes=2, codec='pcm_s16le')\n",
    "    finally:\n",
    "        video.close()\n",
    "\n",
    "# Function to transcribe audio using Whisper with optimized parameters for Vietnamese\n",
    "def transcribe_audio(audio_path, model_name=\"large-v3\", language=\"vi\", task=\"transcribe\", beam_size=5, best_of=5):\n",
    "    \"\"\"\n",
    "    Transcribe audio using a specified Whisper model with optimized parameters for Vietnamese.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        model_name (str): The name of the Whisper model to use (e.g., \"base\", \"small\", \"medium\", \"large\").\n",
    "        language (str): Language code for transcription (default: \"vi\" for Vietnamese).\n",
    "        task (str): Task type, either \"transcribe\" (speech-to-text) or \"translate\" (speech-to-English text).\n",
    "        beam_size (int): Beam search size for better decoding (default: 5).\n",
    "        best_of (int): Number of best candidates to consider during decoding (default: 5).\n",
    "\n",
    "    Returns:\n",
    "        list: List of transcription segments.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the specified Whisper model\n",
    "        model = whisper.load_model(model_name)\n",
    "        \n",
    "        # Transcribe the audio with specified options\n",
    "        result = model.transcribe(\n",
    "            audio_path,\n",
    "            language=language,\n",
    "            task=task,\n",
    "            beam_size=beam_size,\n",
    "            best_of=best_of,\n",
    "            #fp16=False  # Disable FP16 if running on CPU to prevent errors\n",
    "        )\n",
    "        return result[\"segments\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error transcribing audio with Whisper model {model_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "def extract_acoustic_features(audio_path, segments):\n",
    "    \"\"\"\n",
    "    Extract acoustic features for each audio segment.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    features = []\n",
    "    for segment in segments:\n",
    "        start = max(0, int(segment['start'] * sr))\n",
    "        end = min(len(y), int(segment['end'] * sr))\n",
    "        if start >= end:\n",
    "            acoustic_description = \"Invalid time range.\"\n",
    "        else:\n",
    "            audio_segment = y[start:end]\n",
    "            acoustic_description = interpret_acoustic_features(audio_segment, sr)\n",
    "        \n",
    "        features.append({\n",
    "            \"start_time\": segment['start'],\n",
    "            \"end_time\": segment['end'],\n",
    "            \"utterance\": segment['text'],\n",
    "            \"acoustic_features\": acoustic_description\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tổng hợp kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_video(video_path, output_path, query=\"Describe this picture with a 15-word limit sentence. Ignore subtitles in the video. When describing, don’t use 'The image depicts,' start directly, e.g., 'a man in a red shirt...'\"):\n",
    "    \"\"\"\n",
    "    Process a video to extract audio, acoustic features, and frame captions.\n",
    "    \"\"\"\n",
    "    audio_path = \"temp_audio.wav\"\n",
    "    try:\n",
    "        print(\"Extracting audio...\")\n",
    "        extract_audio(video_path, audio_path)\n",
    "\n",
    "        print(\"Transcribing audio...\")\n",
    "        segments = transcribe_audio(audio_path)\n",
    "        print(segments)\n",
    "\n",
    "        print(\"Extracting acoustic features...\")\n",
    "        acoustic_features = extract_acoustic_features(audio_path, segments)\n",
    "\n",
    "        print(f\"Extracting {len(segments)} frames...\")\n",
    "        frames = extract_frames(video_path, num_frames=len(segments))\n",
    "        pil_frames = frames_to_pil(frames)\n",
    "\n",
    "        print(\"Generating visual descriptions...\")\n",
    "        frame_captions = [\n",
    "            call_together_ai_api(frame, query) if frame else \"Error generating caption\"\n",
    "            for frame in pil_frames\n",
    "        ]\n",
    "\n",
    "        results = [\n",
    "            {\n",
    "                \"segment_id\": i,\n",
    "                \"start_time\": feature[\"start_time\"],\n",
    "                \"end_time\": feature[\"end_time\"],\n",
    "                \"utterance\": feature[\"utterance\"],\n",
    "                \"acoustic_features\": feature[\"acoustic_features\"],\n",
    "                \"visual_description\": caption\n",
    "            }\n",
    "            for i, (feature, caption) in enumerate(zip(acoustic_features, frame_captions))\n",
    "        ]\n",
    "\n",
    "        print(f\"Saving results to {output_path}...\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"Processing complete!\")\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_video: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)  # Clean up temporary file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu kết quả thành file JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "video_path = \"/kaggle/input/temp-videos2/189 (2).mp4\"  # Replace with your video path\n",
    "output_path = \"/kaggle/working/output.json\"  # Replace with desired output path\n",
    "result = process_video(video_path, output_path)\n",
    "try:\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T15:18:24.075750Z",
     "iopub.status.busy": "2024-12-22T15:18:24.075510Z",
     "iopub.status.idle": "2024-12-22T15:20:06.691868Z",
     "shell.execute_reply": "2024-12-22T15:20:06.691075Z",
     "shell.execute_reply.started": "2024-12-22T15:18:24.075730Z"
    }
   },
   "source": [
    "# Example usage\n",
    "results = []\n",
    "for i in range(189, 190):\n",
    "    video_path = f\"/kaggle/input/videos/{i}.mp4\"  # Replace with your video path\n",
    "    output_path = f\"/kaggle/working/output_{i}.json\"  # Replace with desired output path\n",
    "    result = process_video(video_path, output_path)\n",
    "    results.append(result)\n",
    "    try:\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T15:20:06.693336Z",
     "iopub.status.busy": "2024-12-22T15:20:06.692856Z",
     "iopub.status.idle": "2024-12-22T15:20:06.929823Z",
     "shell.execute_reply": "2024-12-22T15:20:06.928959Z",
     "shell.execute_reply.started": "2024-12-22T15:20:06.693311Z"
    }
   },
   "source": [
    "!zip -r /kaggle/working/compressed_folder.zip /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/kaggle/working/output.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6243597,
     "sourceId": 10271318,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
